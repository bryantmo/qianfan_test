import os
import appbuilder

# 请前往千帆AppBuilder官网创建密钥，流程详见：https://cloud.baidu.com/doc/AppBuilder/s/Olq6grrt6#1%E3%80%81%E5%88%9B%E5%BB%BA%E5%AF%86%E9%92%A5
os.environ["APPBUILDER_TOKEN"] = 'bce-v3/ALTAK-e8I60qicEUcctvvnFZqNR/0be9cbc0e1ed072db084a7cc3ed3f3ba8675c3b1'

text = '1 前言此前介绍过开源合规引擎的后台实现：《腾讯工蜂技术集——开源合规扫描后台技术实现》，随着业务扫描量的激增，开源合规引擎系统也面临着一些复杂的故障和挑战。 下文我就结合最近的扫描引擎排障工作，跟大家一起探讨如何优化系统的性能、扩展性和容错能力，为读者提供参考和借鉴，以确保系统的高效运行和可靠交付。2 扫描异常与排障思路工蜂代码隐私计算模块的某类任务，出现了长时间处于扫描状态；扫描物料大小是568MB左右，预期能够短段时间内出结果，实际过了6个小时都没有计算完（并且只在该项目物料扫描时稳定复现问题，其他项目不能复现）。2.1 排障思路\
先抛结论：最终我们通过上面的过程进行定位分析：\
排查发现是合规扫描引擎的集群资源吃紧，结合云原生组件kubeproxy反向代理机制，两者结合引发所导\
下面具体列出分析思路和大致流程\
3 排障定位\
3.1 业务流程梳理\
3.1.1 任务流程图\
用户上传源数据包：用户可以上传自己的任务数据包，并可以配置任务执行的所需资源（比如：执行算法、执行线程数等）\
APP1→ APP2：上传任务数据\
任务进入APP2内部队列：优先对进入的任务进行数据分片处理\
APP2→ APP3： APP2分片处理完成之后，按照可配置扫描线程数T，进行按每批次T个请求，将分片内容传输给APP3\
APP3：从磁盘IO读取开源知识库数据\
APP3：对接收到的分片内容，对数据进行算法分析\
APP3： 所有请求携带的分片数据都分析完毕，并且全部正确响应给APP2，宣告：一个任务“完成”\
3.1.2 分析\
既然目前是任务一直扫描，说明问题是出在了（3）~（7）步骤上了，那么聚焦于APP2和APP3。 基于他们的请求响应关系，下文将APP2定位成客户端，将APP3定位成服务端。\
3.2 容器进程分析\
正常的预期现象是：两边容器都有业务进程，并且两边进程频繁进行HTTP通信；当任务扫描结束之后，两边进程都将退出被系统销毁。 那么我们首先需要分析两侧容器进程。\
\
3.2.1 查看容器子进程\
通过ps -ef，分别在客户端APP2和服务端APP3，打印进程状态。\
客户\
客户端APP2的任务进程：有一个进程存活，说明客户端进程卡住了。\
服务端\
服务端APP3的任务进程：没有执行中的任务进程了。\
3.2.2 分析\
定位是客户端APP2的进程卡死，而服务端APP3的进程正常结束了。\
3.3 进程卡死原因定位\
分析进程卡死的原因，首先是想到日志，然后是网络。\
3.3.1 查看容器日志\
在云容器的日志看，发现并没有打印相关的ERROR级别日志，说明业务是整体成功的状态，所以我们更加怀疑是环境问题（网络/IO等资源）导致。\
3.3.2 容器进程的网络端口状态\
通过netstat -ntp| grep PID，分别在APP2和APP3进程关联的网络端口状态。\
客户\
服务端\
由于不存在工作进程，所以也查不出关联的网络端口了。\
3.3.3 分析\
通过网络排查，发现了客户端APP2的进程，存在4个TCP端口一直在监听状态，并没有正常关闭。\
3.4 请求链路分析\
分别从客户端和服务端角度出发，去定位TCP连接异常监听。\
3.4.1 思路\
从客户端APP2角度看\
     1.进程假死原因是：4个TCP连接建立之后，TCP端口一直在等待数据响应（即客户端发起HTTP请求一直阻塞）\
     2.在扫描任务进行中，过程可能发起>8000次请求，最后残留了4个请求异常的TCP连接\
     3.在 3.2.1步骤中 发现：客户端进程是通过 service-name 来请求服务端容器\
从服务端APP3角度看\
     1.虽然计算工作量会很大，但服务端进程最终正常销毁了\
3.4.2 请求链\
由于合规扫描引擎集群是已经部署上云，并且在K8S部署架构下运行，和技术运营的同学一起梳理出以下的请求链路：\
这里与HTTP普通请求响应的区别：由于service的“从中作梗”，kube-proxy其实是一个代理层负责实现service。\
3.4.2.1 kube-proxy\
通过kube-proxy的ipvs机制，实现了从 service-ip 到 容器ip的映射，完成一个网络转发代理，最终实现容器之间的通信。\
3.4.2.2 实际转发请求\
请求链路最终经过了以下步骤：\
容器APP2发起的请求时，首先通过service-name找到APP3-service（service是对外暴露pod的一层代理）\
随后请求经过kube-proxy处理，以实现虚拟 IP 转换（即service-ip到pod实例ip的转换）\
云上的kube-proxy采用了ipvs代理模式，最终实现将流量导向到某一个后端 Pod（即APP3-pod）。\
流量导向完成后，请求最终会进入pod的一个实例（即APP3-容器）\
3.4.3 分析\
上面在 3.3.3步骤 也分析到了，客户端的连接（客户端APP2→APP3-service）是一直建立的，而服务端的连接（APP3-service→APP3-容器）是关闭了的。 那么我们判断问题是在了kube-proxy代理这个环节上。\
3.4.4 猜想验证\
因为恢复业务使用一直是当务之急，所以基于请求链路的理解，我们大胆测试了一下：改为通过pod-ip/port直连通信的方式，客户端进程能否正常结束呢？ 随后验证：该方案是可行的，此时的客户端和服务端进程都正常结束了。\
3.4.4.1 临时解决方案\
通过pod-ip/port直连的方式，同时技术运营同学也辅助了pod重启之后的pod-ip动态刷新的工作，确保临时方案的可用性。 至此，我们优先恢复了业务的正常使用。\
3.4.5 根本问题\
但kube-proxy的流量代理问题，仍旧没定位清晰；未来合规扫描引擎服务，如果要继续做高可用部署，依旧是离不开这个组件的，所以继续盘它。 通过3.4.3步骤 分析，最终定位到问题出在了kube-proxy代理这个环节上，所以决定在客户端和服务端两侧进行抓包。\
3.5 抓包分析网络\
通过tcpdump，我们分别在客户端和服务端里，实现了流量抓包（虽然日志非常大，幸好容器分配到的磁盘空间足够，事后也有清理），随后是下载出来用wireshark分析网络情况。 期间过程有点繁琐，因为要顺序性的启动抓包进程、客户端服务端进程复现、以及文件权限申请等细节，这里不对抓包过程展开。'
tag_extraction = appbuilder.TagExtraction(model="ERNIE Speed-AppBuilder")
result = tag_extraction(appbuilder.Message(text))
print("Answer: \n{}".format(result))

#Answer:
# Message(name=msg, content=1.开源合规引擎
# 2.扫描异常
# 3.排障思路
# 4.业务流程
# 5.容器进程
# 6.进程卡死
# 7.网络排查
# 8.请求链路
# 9.kube-proxy
# 10.抓包分析, mtype=dict, extra={}, token_usage={'prompt_tokens': 1412, 'completion_tokens': 53, 'total_tokens': 1465})
